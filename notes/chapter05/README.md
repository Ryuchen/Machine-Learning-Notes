## 第五章 神经网络

---

这一个章节个人感觉周志华老师的西瓜书对于神经网络的阐述还是要过于科普性一点，大概是为了能够将神经网络发展过程中的几个关键点模型都阐述一遍，受限于篇幅的限制所以比较简单，所以在这里先把书中关键知识点摘录在此，然后同之前一样，将更细的内容补充在后续博文中。

### 5.1 神经元模型

####  概念

**神经网络（neural network）**

神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所做出的的交互反应

**神经元（neuron）**

即神经网络定义中的“简单单元”（unit）

#### M-P神经元模型

![image-20201118112028737](https://cdn.jsdelivr.net/gh/Ryuchen/ImageBed@develop/2020/11/18/8c442fb540fbc352e61a8b5bbe8505ec.webp)

**公式**

$$y=f(\sum_{i}w_ix_i - \theta)$$

**解释**

- 神经元接收到来自 $n$ 个其他神经元传递过来的输入信号
- 其输入信号经过<u>带权重的连接（connection）</u>进行传递
- 神经元接收到的总输入值将与<u>神经元的阈值\theta（threshold）</u>进行比较
- 通过<u>激活函数（activation function）</u>处理以产生神经元的输出 $y$

#### 激活函数

**阶跃函数**

![image-20201118112344559](https://cdn.jsdelivr.net/gh/Ryuchen/ImageBed@develop/2020/11/18/23d33311b4ac8bdd51a686dc8b6b4c7a.webp)

- 将输入值映射为0和1，其中1对应神经元激活，0对应神经元抑制
- 不连续、不光滑的特性
- 这是一种理想状态下的情况

**Sigmoid函数(仅作为一个代表)**

![image-20201118112359079](https://cdn.jsdelivr.net/gh/Ryuchen/ImageBed@develop/2020/11/18/a0f8b0435e1e89e28210c8fbfe27a1fa.webp)

- 将可能在较大范围内变化的输入值，尽可能的挤压到（0，1）的输出值范围内
- 连续、光滑、可微的特性
- 这是一种实际状况下的情况

### 5.2 感知机与多层网络

#### 线性可分（linearly separable）

- 若两类模式是线性可分的，即存在一个线性超平面能将它们分开，则这个问题就是线性可分问题

  - 对于二维（两个特征）的数据集来说，如果存一条直线，能够把这两个分类完美区分，那么这个数据集就是线性可分的。
  - 如果是多维数据，比如有k个特征，如果存在一个维度为k-1的超平面能够完美分开两类的数据点的化，也同样称为线性可分。

- 简单举例

  - 实现简单的逻辑运算：与、非、或
  - 实现复杂的逻辑运算：异或

![image-20201118112555015](https://cdn.jsdelivr.net/gh/Ryuchen/ImageBed@develop/2020/11/18/320062886f1b240ede3c1ff22191e539.webp)

#### 感知机（Perceptron）

由两层神经元组成，输入层接收外界输入信号后传递给输出层，输出层是M-P神经元，亦称“阈值逻辑单元”（threshold logic unit）

![image-20201118112621435](https://cdn.jsdelivr.net/gh/Ryuchen/ImageBed@develop/2020/11/18/df9be4c969225b8f3e1eccda84ca997b.webp)

**公式**

$$y=f(\sum_iw_ix_i - \theta)$$

> PS：这里同MP神经元模型一致
>
> - 给定训练数据集，权重 $w_i$ 和阈值 $\theta$ 可以通过学习得到
> - 阈值 $\theta$ 可以看做一个固定输入，则此时可以看作为一个权重学习模型

**学习过程**

对训练样例 $(\boldsymbol{x},y)$，若当前的感知机的输出为 $\hat{y}$，则其对应的权重 $w_i$ 调整公式

- $w_i \leftarrow w_i + \Delta w_i$
- $\Delta w_i = \eta (y-\hat{y})x_i$

> PS: $x_i$ 是 $\boldsymbol{x}$ 对应于第i个输入神经元的分量； $\eta$ 通常设置为小正数，$\in(0,1)$ 称之为学习率(learning rate)

#### 模型

##### 单层感知机模型

- 只有输出层神经元进行激活函数处理，即只拥有一层功能神经元（functional neuron）
- 可解决线性可分问题

  - 如果是线性可分的，则学习过程一定会收敛（converge）
  - 如果是线性不可分的，则学习过程将会发生振荡（fluctuation）

- 即求适当的权向量 $\boldsymbol{w}=(w_1;w_2;...;w_{n+1})$ 的过程

##### 多层感知机模型(MLP)

- 在输入层（input layer）和输出层（output layer）之间存在隐含层（hidden layer），且隐含层和输出层神经元都是拥有激活函数的功能神经元
- 可解决非线性可分问题

  - 两层感知机就能解决异或可分问题

![image-20201118113433943](https://cdn.jsdelivr.net/gh/Ryuchen/ImageBed@develop/2020/11/18/56af63a1494d7b2ae1391fec10749fe9.webp)

- 即根据训练数据来调整神经元之间的连接权（connection weight）以及每个功能神经元的阈值
- 多层前馈神经网络（multi-layer feedforward neural）

  - 神经元之间不存在同层连接，也不存在跨层连接
  - 前馈指的是网络拓扑结构上面不存在环或回路

![image-20201118113449545](https://cdn.jsdelivr.net/gh/Ryuchen/ImageBed@develop/2020/11/18/aeb871cd957cf0390002e6a271d40f5a.webp)

> PS: 发现很多书上都说多层感知机模型就是神经网络模型，但是这是不准确的，这里后续博文中会阐述

### 5.3 误差逆传播算法

#### 误差逆传播算法（BackPropagation 简称 BP 算法）

**核心思想**

BP 算法是一个迭代学习算法，在迭代的每一轮中采用广义的感知机学习规则对参数进行更新估计，任意参数 $v$ 的更新估计公式为 $v \leftarrow v + \Delta v$

> PS: 看到 shuhuai008 大佬在视频中说过的，整个神经网络模型其实本质是一种错误驱动算法，个人认为这可能就是现在机器学习的最大障碍，本质上统计学习模型占据了机器学习的当今领头羊，所以错误驱动是其不可避免的障碍

![image-20201118114753988](https://cdn.jsdelivr.net/gh/Ryuchen/ImageBed@develop/2020/11/18/b7863dd3e2d83f23b634fb8135c2815b.webp)

**最终目标**

最小化训练集D上的累计误差，即 $E = \frac{1}{m}\sum_{k=1}^m E_k$

![image-20201118115206135](https://cdn.jsdelivr.net/gh/Ryuchen/ImageBed@develop/2020/11/18/fa732a6d4df5181a3d669a474adbe678.webp)

**演算过程**

参数定义

- 给定训练集 $D=\{(\boldsymbol{x}_1,\boldsymbol{y}_1),(\boldsymbol{x}_2,\boldsymbol{y}_2),...,(\boldsymbol{x}_m,\boldsymbol{y}_m)\},\boldsymbol{x}_i \in \mathbb{R}^d,\boldsymbol{y}_i \in \mathbb{R}^l$，即输入示例由 $d$ 个属性描述，输出 $l$ 维实值向量
- 输出层第 $j$ 个神经元的阈值用 $\theta_j$ 表示
- 隐层第 $h$ 个神经元的阈值用 $\gamma_h$ 表示
- 输入层第 $i$ 个神经元与隐层第 $h$ 个神经元之间的连接权为 $v_{ih}$
- 隐层第 $h$ 个神经元与输出层第 $j$ 个神经元之间的连接权为 $w_{hj}$
- 隐层第 $h$ 个神经元的输出用 $b_h$ 表示

因此

- 隐层第 $h$ 个神经元接收到的输入为 $\alpha_h=\sum_{i=1}^d v_{ih}x_i$
- 输出层第 $j$ 个神经元接收到的输入为 $\beta_j=\sum_{h=1}^q w_{hj}b_h$

假设输出层和隐层的神经元都使用Sigmoid函数的情况下

- 则训练例 $(\boldsymbol{x}_k, \boldsymbol{y}_k)$，在神经网络上的输出为 $\hat{\boldsymbol{y}}_k = (\hat{y}_1^k, \hat{y}_2^k, ···, \hat{y}_l^k)$ 即 $\hat{\boldsymbol{y}}_j^k=f(\beta_j-\theta_j)$

- 则对应的当前网络在 $(\boldsymbol{x}_k, \boldsymbol{y}_k)$ 上的均方误差为 $E_k = \frac{1}{2} \sum_{j=1}^l(\hat{y}_j^k - y_j^k)^2$

  > PS: 这里的1/2只是为了后续求导方便

网络参数数量

- 总数目：$(d+l+1)q + l$ = 输出层到隐层 $d \times q$ + 隐层到输出层 $q \times l$ + $q$ 个隐层神经元的阈值 + $l$ 个输出层神经元的阈值

特点

- 每次仅针对一个训练样例更新连接权和阈值，算法的更新规则是基于单个样例的均方误差进行调整的
- 参数更新的非常频繁，而且对不同样例进行更新的效果可能出现"抵消"现象

> PS: 上面的称之为**标准BP算法**

#### 累积BP算法

全称：累积误差逆传播算法 (accumulated error backpropagation)

特点

- 同标准BP算法类似，不过是基于累积误差最小化的更新规则，即每一次都遍历完整个数据集后再更新参数

  > PS: 这里的整个数据集并不是指所有的样本数据，而是一个one epoch中的数据量

- 直接针对训练集的累计误差最小化优化

- 累积误差下降到一定程度后，进一步下降会非常缓慢

---

**Hornik 在 1989年证明，只需一个包含足够多神经元的隐层，多层前馈神经网络就能以任意精度逼近任意复杂度的连续函数**

> PS：如何设置隐层神经元的个数仍是个未决问题，实际应用中通常靠试错法（trial-by-error）来进行调整

---

> **PS: 这里引发了一个非常有意思的问题，"so why deep？"**

---

#### 过拟合问题

**起因**：BP神经网络强大的表示能力

**现象**：训练误差持续降低，但测试误差却可能上升

**解决策略**

早停（early stopping）

- 将数据集分成训练集和验证集

  - 训练集用来计算梯度、更新连接权和阈值
  - 验证集用来估计误差

- 若训练集误差降低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值

正则化（regularization）

- 在误差目标函数中增加一个用于描述网络复杂度的部分
- 举例

  - 增加连接权和阈值的平方和
  - $E = \lambda \frac{1}{m} \sum_{k=1}^m E_k + (1-\lambda)\sum_{i}w_i^2$

    - 其中的 $\lambda \in (0,1)$ 用于对经验误差与网络复杂度这两项进行折中，常通过交叉验证法来估计

### 5.4 全局最小与局部极小 

![image-20201118130703475](https://cdn.jsdelivr.net/gh/Ryuchen/ImageBed@develop/2020/11/18/838c69a93b2ce93af2fa765f3ada6779.webp)

#### 对比

##### 数学定义

对于 $\boldsymbol{w}^*$ 和 $\theta^*$，若存在 $\epsilon > 0$ 使得 
$$
\forall (\boldsymbol{w};\theta) \in 
\{
(\boldsymbol{w};\theta) |
\lVert
(\boldsymbol{w};\theta) - (\boldsymbol{w}^*;\theta^*)
\rVert
\le \epsilon
\}
$$

都有 $E(\boldsymbol{w};\theta) \ge E(\boldsymbol{w}^*;\theta^*)$ 成立，则 $(\boldsymbol{w};\theta)$ 为**局部极小解**

---

若对参数空间中的任意 $ (\boldsymbol{w};\theta)$，都有 $E(\boldsymbol{w};\theta) \ge E(\boldsymbol{w}^*;\theta^*)$ 成立，则 $(\boldsymbol{w};\theta)$ 为**全局最小解**

##### 直观体现

- 局部极小解是参数空间中的某个点，其领域点的误差函数数值均小于该点的函数值
- 全局最小解则是指参数空间中所有点的误差函数值均不小于该店的误差函数值

##### 两者对应的误差函数的取值名称

$$
E(\boldsymbol{w}^*;\theta^*)
$$

- 局部极小值（local minimum）
- 全局最小值（global minimum）

##### 关联

- 可能存在多个局部极小值，却只有一个全局最小值
- 可以说“全局最小”一定是“局部极小”，反之则不成立

#### 基于梯度的搜索

> PS: 最广泛的参数寻优方法

- 从某些初始解出发，迭代寻找最优参数值
- 步骤

  - 先计算误差函数在当前点的梯度
  - 然后根据梯度确定搜索方向

    - 负梯度方向是函数值下降最快的方向

- 如果误差函数在当前点的梯度为零，则已达到局部极小，更新量将为零，这意味着参数的迭代更新将在此停止

#### 如何跳出局部极小？

- 多个出发点

  - 以多组不同参数初始化多个神经网络，选取其中误差最小的解作为最终参数

- 模拟退火（simulated annealing）

  - 在每一步都以一定的概率接受比当前解更差的结果，从而有助于跳出局部极小解，注意的是每次迭代过程中，接受的概率都要随着时间的推移而逐渐下降

- 随机梯度下降

  - 与标准梯度下降法精度计算梯度不同，随机梯度下降法在计算梯度时加入了随机因素，因此即便陷入局部极小值，它计算出来的梯度也可能不为零

- 遗传算法（genetic algorithm）

### 5.5 其他常见神经网络

#### RBF网络

##### 概念

- Radial Basis Function 径向基函数网络

- 是一种单隐层前馈神经网络，它使用径向基函数作为隐层神经元激活函数，而输出层是对隐层神经元输出的线性组合

  - >  PS: 理论上来说可以使用多个隐层

- 假定输入为 $d$ 维向量 $\boldsymbol{x}$ ，输出则为实数

##### 公式

- $\varphi(\boldsymbol{x})=\sum_{i=1}^qw_i\rho(\boldsymbol{x},\boldsymbol{c}_i)$

  - $q$ 为隐层神经元个数
  - $\boldsymbol{c}_i$ 和 $w_i$ 分别是第 $i$ 个隐层神经元所对应的中心和权重
  - $\rho(\boldsymbol{x},\boldsymbol{c}_i)$ 是径向基函数

> 补充概念：径向基函数
>
> - 某种沿径向对称的标量函数，通常定义为样本 $\boldsymbol{x}$ 到数据中心 $\boldsymbol{c}_i$ 之间的欧式距离的单调函数
> - 常用高斯径向基函数: $\rho(\boldsymbol{x},\boldsymbol{c}_i)=e^{-\beta_i\lVert \boldsymbol{x} - \boldsymbol{c}_i \rVert ^2}$

> PS: 具有**足够多隐层神经元**的RBF网络能以任意精度逼近任意连续函数

##### 训练步骤

- Step1：确定神经元中心 $\boldsymbol{c}_i$ ，常用方法包括：随机采样、聚类等
- Step2：利用BP算法等来确定参数 $w_i$ 和 $\beta_i$ 的值

#### ART网络

##### 概念

- Adaptive Resonance Theory 自适应谐振理论网络
- 该网络由比较层、识别层、识别阈值和重置模块构成

  - 比较层负责接收输入样本，并将其传递给识别层神经元
  - 识别层每个神经元对应一个模式类，神经元数目可在训练过程中动态增长以增加新的模式类
  - 在接收到比较层输入信号后，识别层神经元之间相互竞争以产生获胜神经元
  - 获胜神经元向其他识别层神经元发出信号，抑制其激活

##### 流程

- 若输入向量与获胜神经元所对应的代表向量之间的相似度大于识别阈值，则当前输入样本将被归为该代表向量所属类别，同时，网络连接权将会被更新，使得以后再接收到相似输入时该模式类会计算出更大的相似度
- 若相似度不大于识别阈值，则重置模块将在识别层增设一个新的神经元，其代表向量就设置为当前输入向量

> 补充概念：**竞争型学习**（competitive learning）
>
> - 神经网络中一种常用的无监督学习策略
> - 网络的输出神经元相互竞争，每一时刻仅有一个竞争获胜的神经元被激活，其他神经元的状态被抑制
> - 常被称作“胜者通吃”（winner-take-all）原则

##### 特点

ART网络比较好的缓解了竞争型学习中的“**可塑性-稳定性窘境**”（stability-plasticity dilemma）

- 可塑性：是指神经网络要有学习新知识的能力
- 稳定性：是指神经网络在学习新知识时要保持对就知识的记忆

- 可进行**增量学习**（incremental learning）和**在线学习**（online learning）
  - 增量学习：是指在学得模型后，再接收到训练样例时，仅需根据新样例对模型进行更新，不必重新训练整个模型，并且先前学的有效信息不会被”冲掉“
  - 在线学习：是指每获得一个新样本就进行一次模型更新，显然，在线学习室增量学习的特例，而增量学习可视为”批模式“的在线学习

##### 家庭族

- 网络处理实值输入的ART2
- 结合模糊处理的FuzzyART网络
- 可进行监督学习的ARTMAP网络

#### SOM网络

##### 概念

- Self-Organizing Map 自组织映射网络
- 是一种竞争学习型的无监督神经网络，它能将高维输入数据映射到低维空间（通常为二维），同时保持输入数据在高维空间的拓扑结构，即将高维空间中相似的样本点映射到网络输出层中的邻近神经元

##### 组成

![image-20201118150849174](https://cdn.jsdelivr.net/gh/Ryuchen/ImageBed@develop/2020/11/18/c103ef70a36510d7d5129875c7a0398f.webp)

- SOM网络中的输出层神经元以矩阵方式排列在二维空间中，每个神经元都拥有一个权向量
- 网络在接收输入向量后，将会确定输出层获胜神经元，它决定了该输入向量在低维空间中的位置

- SOM的训练目标就是为每个输出层神经元找到合适的权向量，以达到保持拓扑结构的目的

##### 流程

- 在接收到一个训练样本后，每个输出层神经元会计算该样本与自身携带的权向量之间的距离，距离最近的神经元将成为竞争获胜者，称为最佳匹配单元（best matching unit）
- 然后，最佳匹配单元及其邻近神经元的权向量将被调整，以使得这些权向量与当前输入样本的距离缩小

#### 级联相关网络

##### 概念

- Cascade-Correlation 级联相关网络
- 结构自适应神经网络的重要代表

##### 结构自适应神经网络

- 结构自适应网络则将网络结构也当作学习的目标之一，并希望能在训练过程中找到最符合数据特点的网络结构
- 结构自适应神经网络也称为构造性神经网络（constructive network）
- ART网络因为在训练过程中隐层神经元数目可变因此也是一种结构自适应神经网络

##### 组成

![image-20201118150957148](https://cdn.jsdelivr.net/gh/Ryuchen/ImageBed@develop/2020/11/18/0b2fb766d86f6f215ed262efd1086d7b.webp)

- 级联：指建立层次连接的层级结构

- 相关：指通过最大化新神经元的输出与网络误差之间的相关性（correlation）来训练相关的参数


##### 流程

- 在开始训练时，网络只有输入层和输出层，处于最小拓扑结构，随着训练的进行，新的隐层神经元逐渐加入，从而创建起层级结构
- 当新的隐层神经元加入时，其输入端连接权值是冻结固定的

#### Elman网络

##### 递归神经网络（recurrent neural networks）

- 允许网络中出现环形结构，从而可让一些神经元的输出反馈回来作为输入信号
- 这样的结构和信息反馈过程，使得网络在t时刻输出状态不仅与t时刻的输入有关，还与t-1时刻的网络状态有关，从而能处理与时间有关的动态变化

##### 组成

![image-20201118151350409](https://cdn.jsdelivr.net/gh/Ryuchen/ImageBed@develop/2020/11/18/4f0fc23896dc9c890b11dc08d23f9b35.webp)

- 隐层神经元的输出被反馈回来，与下一时刻输入层神经元提供的信号一起，作为隐层神经元在下一时刻的输入
- 隐层神经元通常采用Sigmoid函数
- 网络的训练则常通过推广的BP算法

#### Boltzmann机

##### 概念

- Boltzmann Machine，Boltzmann机
- 是一种“基于能量的模型”（energy-based model）

##### 基于能量的模型（energy-based model）

- 为网络状态定义一个能量（energy），能量最小化时网络达到理想状态
- 网络的训练就是在最小化这个能量函数

##### 组成

- ![image-20201118151456715](https://cdn.jsdelivr.net/gh/Ryuchen/ImageBed@develop/2020/11/18/beedb8dc33d532f5c38a48a8328c4c48.webp)
- 显层：用于数据的输入与输出

- 隐层：则被理解为数据的内在表达

- Boltzmann机中的神经元都是布尔型的，即只能取0、1两种状态，状态1表示激活，状态0表示抑制

##### Restricted Boltzmann Machine（RBM，受限Boltzmann 机）

- 由于Boltzmann机的训练过程就是讲每个训练样本视为一个状态向量，使其出现的概率尽可能大
- 标准的Boltzmann机是全连接图，训练网络的复杂度很高，目前也没有研究证明其对现实问题有实际意义
- 仅保留显层与隐层之间的连接，从而将Boltzmann机结构简化为二部图

### 5.6 深度学习

#### 背景

- 理论上来说，参数越多的模型复杂度越高，容量越大，这意味着它能完成更复杂的学习任务
- 一般情形下，复杂模型的训练效率低，易陷入过拟合
- 计算能力的大幅提高可缓解训练低效性，训练数据的大幅增加则可降低过拟合风险

#### 典型的深度学习模型就是很深层的神经网络

- 提高学习器的容量就是增加模型的复杂度
- 从增加模型的复杂度来看，增加隐层的数目显然比增加隐层神经元的数目更有效
- 因为增加隐层数不仅增加了拥有激活函数的神经元数目，还增加了激活函数嵌套的层数

#### 产生的问题

使用经典算法进行训练时，因为误差在多隐层内逆传播时，往往会发散而不嗯呢该收敛达到稳定状态

#### 解决办法

- 无监督逐层训练（unsupervised layer-wise training）

  - 预训练（Pre-training）：每次训练一层隐结点，训练时将上一层隐结点的输出作为输入，而本层隐结点的输出作为下一层隐结点的输入
  - 调优（Fine-turning）：在预训练全部完成后，在对整个网络进行微调训练

- 权共享（weight sharing）

  - 让一组神经元使用相同的连接权
  - 典型代表CNN（Convolutional Nerual Network 卷积神经网络）

#### 特征学习（feature learning）或表示学习（representation learning）

逐渐将初始的“低层”特征表示转化为“高层”特征表示，然后用“简单模型”即可完成复杂的分类等学习任务

